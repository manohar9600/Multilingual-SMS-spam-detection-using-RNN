{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T10:05:09.444899Z",
     "start_time": "2018-03-20T10:05:05.358816Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T10:05:18.395408Z",
     "start_time": "2018-03-20T10:05:17.381028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('Resources/spam.csv', encoding = 'latin-1')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
    "data1 = data1.drop(list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Dear Customer, +916300623587 is now available ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Dear Customer, You have a missed call from +91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Join Hike to get Rs 40. Earn upto Rs. 10,000 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Just sent you some money and invited you to Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Just sent you some money and invited you to Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code label                                                msg\n",
       "0     1   ham  Dear Customer, +916300623587 is now available ...\n",
       "1     1   ham  Dear Customer, You have a missed call from +91...\n",
       "2     1  spam  Join Hike to get Rs 40. Earn upto Rs. 10,000 b...\n",
       "3     1  spam  Just sent you some money and invited you to Hi...\n",
       "4     1  spam  Just sent you some money and invited you to Hi..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.ExcelFile('Resources/indiandataset1.xls')\n",
    "data2 = data2.parse(0)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for english ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "i = 0\n",
    "for row in data2.itertuples():\n",
    "    if not int(row[1]) == 1:\n",
    "        lst.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n"
     ]
    }
   ],
   "source": [
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_eng = data2.drop(data2.index[lst], axis = 0) # dropped non engish rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_eng = pd.concat([data1, data2_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_eng['msg'], data_eng['label'], test_size = 0.30)\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing   ->  __splitting, removing punctuation, lowering (Lower Casing)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manohar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "temp = []\n",
    "for i in range(len(X_train)):\n",
    "    words = []\n",
    "    rev = str(X_train.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "X_train = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Words to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 3000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manohar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 175, 175)          525000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               291648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 817,042\n",
      "Trainable params: 817,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "embed_dim = X_train.shape[1]\n",
    "lstm_out = 196\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6521/6521 [==============================] - 99s 15ms/step - loss: 0.1699 - acc: 0.9368\n",
      "Epoch 2/5\n",
      "6521/6521 [==============================] - 89s 14ms/step - loss: 0.0601 - acc: 0.9802\n",
      "Epoch 3/5\n",
      "6521/6521 [==============================] - 85s 13ms/step - loss: 0.0367 - acc: 0.9880\n",
      "Epoch 4/5\n",
      "6521/6521 [==============================] - 83s 13ms/step - loss: 0.0339 - acc: 0.9893\n",
      "Epoch 5/5\n",
      "6521/6521 [==============================] - 95s 15ms/step - loss: 0.0202 - acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cb814cc88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 5,batch_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(X_test)):\n",
    "    words = []\n",
    "    rev = str(X_test.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(temp)\n",
    "le = X_train.shape[1]\n",
    "temp2 = [[0 for i in range(le)] for j in range(len(temp))]\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[i])-1,-1,-1):\n",
    "        temp2[i][le-1+j-len(temp[i])+1] = temp[i][j];\n",
    "\n",
    "temp2 = np.array(temp2).reshape(len(temp),le,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2219   39]\n",
      " [  55  483]]\n"
     ]
    }
   ],
   "source": [
    "Y_pr = model.predict(temp2)\n",
    "\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(len(Y_pr)):\n",
    "    if Y_pr[i][0] >= Y_pr[i][1]:\n",
    "        Y_pred.append('ham')\n",
    "    else:\n",
    "        Y_pred.append('spam')\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)  \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrongly Predicted to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "file = open('output/wrongly_predicted_eng.csv', 'w')\n",
    "wr_file = csv.writer(file)\n",
    "wr_file.writerow([\"label\",\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_test)):    \n",
    "    if not Y_test.iloc[i] == Y_pred[i]:\n",
    "        wr_file.writerow([Y_test.iloc[i],X_test.iloc[i]])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Hindi .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "i = 0\n",
    "for row in data2.itertuples():\n",
    "    if not int(row[1]) == 2:\n",
    "        lst.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hin = data2.drop(data2.index[lst], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_hin['msg'], data_hin['label'], test_size = 0.30)\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "temp = []\n",
    "for i in range(len(X_train)):\n",
    "    words = []\n",
    "    rev = str(X_train.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "X_train = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 600\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manohar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 35, 35)            21000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               181888    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 203,282\n",
      "Trainable params: 203,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "embed_dim = X_train.shape[1]\n",
    "lstm_out = 196\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6723 - acc: 0.7850\n",
      "Epoch 2/12\n",
      "107/107 [==============================] - 0s 3ms/step - loss: 0.5133 - acc: 0.8131\n",
      "Epoch 3/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.4322 - acc: 0.8131\n",
      "Epoch 4/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.3820 - acc: 0.8131\n",
      "Epoch 5/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.3051 - acc: 0.8692\n",
      "Epoch 6/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.2069 - acc: 0.9626\n",
      "Epoch 7/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.1320 - acc: 0.9626\n",
      "Epoch 8/12\n",
      "107/107 [==============================] - 0s 3ms/step - loss: 0.0969 - acc: 0.9720\n",
      "Epoch 9/12\n",
      "107/107 [==============================] - 0s 3ms/step - loss: 0.0827 - acc: 0.9813\n",
      "Epoch 10/12\n",
      "107/107 [==============================] - 0s 3ms/step - loss: 0.0551 - acc: 1.0000\n",
      "Epoch 11/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.0237 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cb8949358>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 12,batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(X_test)):\n",
    "    words = []\n",
    "    rev = str(X_test.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(temp)\n",
    "le = X_train.shape[1]\n",
    "temp2 = [[0 for i in range(le)] for j in range(len(temp))]\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[i])-1,-1,-1):\n",
    "        temp2[i][le-1+j-len(temp[i])+1] = temp[i][j];\n",
    "\n",
    "temp2 = np.array(temp2).reshape(len(temp),le,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1]\n",
      " [ 1 35]]\n"
     ]
    }
   ],
   "source": [
    "Y_pr = model.predict(temp2)\n",
    "\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(len(Y_pr)):\n",
    "    if Y_pr[i][0] >= Y_pr[i][1]:\n",
    "        Y_pred.append('ham')\n",
    "    else:\n",
    "        Y_pred.append('spam')\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)  \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrongly predicted to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "file = open('output/wrongly_predicted_hin.csv', 'w')\n",
    "wr_file = csv.writer(file)\n",
    "wr_file.writerow([\"label\",\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_test)):    \n",
    "    if not Y_test.iloc[i] == Y_pred[i]:\n",
    "        wr_file.writerow([Y_test.iloc[i],X_test.iloc[i]])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for Code Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "i = 0\n",
    "for row in data2.itertuples():\n",
    "    if not int(row[1]) == 3:\n",
    "        lst.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_mix = data2.drop(data2.index[lst], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_mix['msg'], data_mix['label'], test_size = 0.30)\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "temp = []\n",
    "for i in range(len(X_train)):\n",
    "    words = []\n",
    "    rev = str(X_train.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "X_train = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 600\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manohar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 35, 35)            21000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 196)               181888    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 203,282\n",
      "Trainable params: 203,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "embed_dim = X_train.shape[1]\n",
    "lstm_out = 196\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.6688 - acc: 0.8852\n",
      "Epoch 2/12\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5652 - acc: 0.8443\n",
      "Epoch 3/12\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4629 - acc: 0.9098\n",
      "Epoch 4/12\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4742 - acc: 0.8443\n",
      "Epoch 5/12\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3205 - acc: 0.9754\n",
      "Epoch 6/12\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2174 - acc: 0.9426\n",
      "Epoch 7/12\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1171 - acc: 0.9836\n",
      "Epoch 8/12\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1003 - acc: 1.0000\n",
      "Epoch 9/12\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0422 - acc: 0.9918\n",
      "Epoch 10/12\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0445 - acc: 0.9836\n",
      "Epoch 11/12\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0243 - acc: 0.9918\n",
      "Epoch 12/12\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0236 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cb89408d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 12,batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(X_test)):\n",
    "    words = []\n",
    "    rev = str(X_test.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(temp)\n",
    "le = X_train.shape[1]\n",
    "temp2 = [[0 for i in range(le)] for j in range(len(temp))]\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[i])-1,-1,-1):\n",
    "        temp2[i][le-1+j-len(temp[i])+1] = temp[i][j];\n",
    "\n",
    "temp2 = np.array(temp2).reshape(len(temp),le,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  0]\n",
      " [ 1 20]]\n"
     ]
    }
   ],
   "source": [
    "Y_pr = model.predict(temp2)\n",
    "\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(len(Y_pr)):\n",
    "    if Y_pr[i][0] >= Y_pr[i][1]:\n",
    "        Y_pred.append('ham')\n",
    "    else:\n",
    "        Y_pred.append('spam')\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)  \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrongly predicted to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "file = open('output/wrongly_predicted_mix.csv', 'w')\n",
    "wr_file = csv.writer(file)\n",
    "wr_file.writerow([\"label\",\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_test)):    \n",
    "    if not Y_test.iloc[i] == Y_pred[i]:\n",
    "        wr_file.writerow([Y_test.iloc[i],X_test.iloc[i]])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "i = 0\n",
    "for row in data2.itertuples():\n",
    "    if not int(row[1]) == 4:\n",
    "        lst.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tel = data2.drop(data2.index[lst], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_tel['msg'], data_tel['label'], test_size = 0.30)\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "temp = []\n",
    "for i in range(len(X_train)):\n",
    "    words = []\n",
    "    rev = str(X_train.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "X_train = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manohar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 69, 69)            13800     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               208544    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 222,738\n",
      "Trainable params: 222,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "embed_dim = X_train.shape[1]\n",
    "lstm_out = 196\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "364/364 [==============================] - 6s 17ms/step - loss: 0.3489 - acc: 0.9560\n",
      "Epoch 2/10\n",
      "364/364 [==============================] - 5s 13ms/step - loss: 0.1381 - acc: 0.9698\n",
      "Epoch 3/10\n",
      "364/364 [==============================] - 5s 13ms/step - loss: 0.1340 - acc: 0.9698\n",
      "Epoch 4/10\n",
      "364/364 [==============================] - 5s 14ms/step - loss: 0.1199 - acc: 0.9698\n",
      "Epoch 5/10\n",
      "364/364 [==============================] - 5s 14ms/step - loss: 0.1086 - acc: 0.9698\n",
      "Epoch 6/10\n",
      "364/364 [==============================] - 5s 13ms/step - loss: 0.0939 - acc: 0.9698\n",
      "Epoch 7/10\n",
      "364/364 [==============================] - 5s 14ms/step - loss: 0.0696 - acc: 0.9808\n",
      "Epoch 8/10\n",
      "364/364 [==============================] - 5s 14ms/step - loss: 0.0626 - acc: 0.9808\n",
      "Epoch 9/10\n",
      "364/364 [==============================] - 5s 13ms/step - loss: 0.0579 - acc: 0.9863\n",
      "Epoch 10/10\n",
      "364/364 [==============================] - 5s 12ms/step - loss: 0.0599 - acc: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ca54e8630>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 10,batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(X_test)):\n",
    "    words = []\n",
    "    rev = str(X_test.iloc[i])\n",
    "    rev = text_to_word_sequence(rev)\n",
    "    temp.append(rev)\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(temp)\n",
    "le = X_train.shape[1]\n",
    "temp2 = [[0 for i in range(le)] for j in range(len(temp))]\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[i])-1,-1,-1):\n",
    "        temp2[i][le-1+j-len(temp[i])+1] = temp[i][j];\n",
    "\n",
    "temp2 = np.array(temp2).reshape(len(temp),le,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152   0]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "Y_pr = model.predict(temp2)\n",
    "\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(len(Y_pr)):\n",
    "    if Y_pr[i][0] >= Y_pr[i][1]:\n",
    "        Y_pred.append('ham')\n",
    "    else:\n",
    "        Y_pred.append('spam')\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)  \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrongly predicted to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "file = open('output/wrongly_predicted_tel.csv', 'w')\n",
    "wr_file = csv.writer(file)\n",
    "wr_file.writerow([\"label\",\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_test)):    \n",
    "    if not Y_test.iloc[i] == Y_pred[i]:\n",
    "        wr_file.writerow([Y_test.iloc[i],X_test.iloc[i]])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
